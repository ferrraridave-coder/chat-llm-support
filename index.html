<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chat LLM — Support</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, sans-serif;
            background-color: #0d0b22;
            color: #ffffff;
            padding: 20px;
            line-height: 1.6;
            max-width: 700px;
            margin: auto;
        }
        h1, h2, h3 {
            color: #cfcfff;
        }
        a {
            color: #89b4ff;
        }
    </style>
</head>
<body>
<h1>Chat LLM — Support</h1>
<p>
If you need help with the Chat LLM app, model configuration, API keys, or any other issue,
you can contact support directly at:<br>
<strong>Email:</strong> <a href="mailto:ferrraridave@icloud.com">ferrraridave@icloud.com</a>
</p>
<p>
Useful links:<br>
- <a href="privacy-policy.html">Privacy Policy</a><br>
- <a href="terms.html">Terms of Use</a>
</p>

<h2>Frequently Asked Questions</h2>

<h3>What is Chat LLM?</h3>
<p>
Chat LLM is a privacy-focused AI chat app with a built-in local model (Llama 3.2 1B) that works
completely offline. The app also optionally supports connecting your own API keys to access 500+
additional cloud models via providers such as OpenRouter and OpenAI.
</p>

<h3>Do I need an account or API key?</h3>
<p>
No. The app includes a built-in local AI model that works immediately without any account, API key,
or internet connection. If you want to access additional cloud models, you can optionally add your
own OpenRouter or OpenAI API keys, which requires an account with those providers.
</p>

<h3>What models are supported?</h3>
<p>
<strong>Built-in:</strong> Llama 3.2 1B runs locally on your device with complete privacy.<br><br>
<strong>Optional Cloud Models:</strong> When you add your own API keys, Chat LLM supports a wide range
of OpenRouter and OpenAI models, including text and vision models. You can send text prompts, images,
and documents to these cloud models.
</p>

<h3>How is my data handled?</h3>
<p>
<strong>Local Model:</strong> When using the built-in local model, all conversations remain on your device.
No data is transmitted anywhere, and the app works completely offline.<br><br>
<strong>Cloud Models (Optional):</strong> When using cloud models with your own API keys, your prompts, images,
and documents are sent securely to the AI provider you selected (OpenRouter or OpenAI) to generate responses.
Chat LLM does not store your prompts or uploads on our servers. For details on how cloud providers handle data,
please refer to their privacy policies.
</p>

<h3>Can I use the app offline?</h3>
<p>
Yes. The built-in local model works completely offline with no internet connection required.
Cloud models require an internet connection to communicate with external AI providers.
</p>

<h3>How do I report a bug?</h3>
<p>
Please email a description of the problem and, if possible, include screenshots or steps to reproduce it.<br>
<strong>Email:</strong> <a href="mailto:ferrraridave@icloud.com">ferrraridave@icloud.com</a>
</p>
</body>
</html>
